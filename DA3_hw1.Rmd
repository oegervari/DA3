---
title: "DA3 - HW1"
author: "Oszkar Egervari"
date: "1/26/2022"
output: 
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
# Set graph size
knitr::opts_chunk$set(echo = FALSE, out.width = "50%" )#fig.asp = 0.5, fig.width = 7, out.width = "90%" )
rm(list=ls())
# Libraries
library(tidyverse)
library(fixest)
library(caret)
library(modelsummary)
library(grid)
library(kableExtra)
# Getting the data
data <- read.csv( 'https://osf.io/4ay9x/download', stringsAsFactors = TRUE)
```

## Introduction

In this report I showcase 4 models, that predict the earnings per hour of Educational Administrators (occupational code 0230). The models have different levels of complexity, model 1 being the most simple, while model 4 contains the most predictor variables. The aim of this report is to find the model with the best performance based on RMSE in full sample, cross-validated RMSE and BIC in full sample. 


## Data

The used for the report is the [cps-earnings dataset](https://osf.io/4ay9x/).
As mentioned above, I chose the Educational Administrators (occupational code 0230) [occupation](https://osf.io/57n9q/) for the prediction. 


```{r, echo=FALSE}
# selecting occupation 230 - Education administrators
data <- data %>% filter(occ2012 == 230)

##### adding variables ####
data <- data %>% mutate(female=as.numeric(sex==2)) %>%
  mutate(w=earnwke/uhours) %>%
  mutate(agesq=age^2) %>% 
  mutate(unionmember=ifelse(unionmme=='Yes',1,0))
```

The number of observations is `r sum(!is.na(data$X))` for all of our key variables.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }
# Linear regressions
model1 <- as.formula(w ~ age + agesq)
model2 <- as.formula(w ~ age + agesq + female)
model3 <- as.formula(w ~ age + agesq + female + grade92)
model4 <- as.formula(w ~ age + agesq + female + grade92 + unionmember + marital + ownchild)

# Running simple OLS
reg1 <- feols(model1, data=data, vcov = 'hetero')
reg2 <- feols(model2, data=data, vcov = 'hetero')
reg3 <- feols(model3, data=data, vcov = 'hetero')
reg4 <- feols(model4, data=data, vcov = 'hetero')

# evaluation of the models
fitstat_register("k", function(x){length( x$coefficients ) - 1}, "No. Variables")
etable( reg1 , reg2 , reg3 , reg4 , fitstat = c('bic','rmse','n','k') )

```

DESCRIPTION OF THE FIGURE. WHAT DOES IT TELS US?

(May change the order of descriptive stats and graph.)

The key pattern of association is:

```{r, echo=FALSE, warning=FALSE, fig.width=4, fig.height = 3, fig.align="center" }
chck_sp <- function( x_var , x_lab ){
  ggplot( df , aes(x = x_var, y = score4)) +
    geom_point(color='red',size=2,alpha=0.6) +
    geom_smooth(method="loess" , formula = y ~ x )+
    labs(x = x_lab, y = "Averaged values of test scores") +
    theme_bw()
}
# Our main interest: student-to-teacher ratio:
chck_sp(df$stratio,'Student-to-teacher ratio')
```

How will you include this in your model?

Short description on the other variables: 2-10 sentence depends on the amount of variables you have. You should reference your decisions on the graphs/analysis which are located in the appendix.

## Model


```{r, echo = FALSE }
# reg1: NO control, simple linear regression
reg1 <- feols( score4 ~ stratio , data = df , vcov = 'hetero' )
# reg2: NO controls, use piecewise linear spline(P.L.S) with a knot at 18
reg2 <- feols( score4 ~ lspline( stratio , 18 ) , data = df , vcov = 'hetero' )
# reg3: control for english learners dummy (english_d) only. 
#   Is your parameter different? Is it a confounder?
df <- df %>% mutate( english_d = 1*(english>1))
reg3 <- feols( score4 ~ lspline( stratio , 18 ) + english_d, data = df , vcov = 'hetero' )
##
# reg4: reg3 + Schools' special students measures (lunch with P.L.S, knot: 15; and special)
reg4 <- feols( score4 ~ lspline( stratio , 18 ) + english_d 
                   + lspline(lunch,15) + special , data = df , vcov = 'hetero' )
#
# reg5: reg4 + salary with P.L.S, knots at 35 and 40, exptot, log of income and scratio
reg5 <- feols( score4 ~ lspline( stratio , 18 ) + english_d
                   + lspline(lunch,15) + special 
                   + lspline(salary,c(35,40)) + exptot 
                   + log( income ) + scratio , data = df , vcov = 'hetero' )
# Naming the coefficients for pretty output
alpha  <- round( reg5$coeftable[1,1] , 2 )
b1 <- round( reg5$coeftable[2,1] , 2 )
b2 <- round( reg5$coeftable[3,1] , 2 )
```

My preferred model is:

score = $`r alpha`$ $`r b1`$ $( student/teacher < 18)$ $`r b2`$ $( student/teacher \geq 18) + \delta Z$

where $Z$ are standing for the controls, which includes controlling for english language, lunch, other special characteristics and wealth measures. From this model we can infer:

- when every covariates are zero, students expected to have grade score of $`r alpha`$
- when the student to teacher is one unit larger, but below the value of 18, we see students to have on average $`r abs(b1)`$ smaller grades.
- when the student to teacher is one unit larger, with the value above or equal to 18, we see students to have on average $`r abs(b2)`$ smaller grades.

However, based on the heteroskedastic robust standard errors, these results are statistically non different from zero. To show that, I have run a two-sided hypothesis test:
$$H_0:=\beta_1 = 0$$
$$H_A:=\beta_1 \neq 0$$
I have the t-statistic as `r round( reg5$coeftable[2,3] , 2 )` and the p-value as `r round( reg5$coeftable[2,4] , 2 )`, which confirms my conclusion.

We compare multiple models to learn about the stability of the parameters. Bla-bla:

```{r, echo = FALSE }
##
# Summarize our findings:
varname_report <- c("(Intercept)" = "Intercept",
                   "stratio" = "student/teacher",
                   "lspline(stratio,18)1" = "student/teacher (<18)",
                   "lspline(stratio,18)2" = "student/teacher (>=18)",
                   "english_d" = "english_dummy")
groupConf <- list("English" = c("english"),
                  "Lunch" = c("lunch"),
              "Other Special" = c("special"),
              "Wealth Measures" = c("exptot","income","scratio"))
vars_omit <- c("english|lunch|special|salary|exptot|income|scratio")
# Note: coefstat = 'confint' is just an example, usually you need to report se.
style_noHeaders = style.tex(var.title = "", fixef.title = "", stats.title = " ")
kable( etable( reg1 , reg2 , reg3 , reg4 , reg5 ,
        title = 'Average test scores for 4th graders',
        dict = varname_report,
        drop = vars_omit ,
        group = groupConf ,
        se.below = T,
        coefstat = 'se',
        fitstat = c('n','r2'),
        se.row = F,
        depvar = F ) , 
        col.names = c('(1)','(2)','(3)','(4)','(5)'),
       "latex", booktabs = TRUE,  position = "H",
       caption = 'Models to uncover relation between test score and student to teacher ratio') %>% kable_styling(latex_options = c("hold_position","scale_down"))
```





## Robustness check / 'Heterogeneity analysis'

Task: calculate and report t-tests for each countries. 



## Conclusion

HERE COMES WHAT WE HAVE LEARNED AND WHAT WOULD STRENGHTEN AND WEAKEN OUR ANALYSIS.

## Appendix

Here comes all the results which are referenced and not essential for understanding the MAIN results.