---
title: "DA3 - HW1"
author: "Oszkar Egervari"
date: "1/26/2022"
output: 
  pdf_document:
    extra_dependencies: ["float"]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos = "!H", out.extra = "")
# Set graph size
knitr::opts_chunk$set(echo = FALSE, out.width = "50%" )#fig.asp = 0.5, fig.width = 7, out.width = "90%" )
rm(list=ls())
# Libraries
library(tidyverse)
library(fixest)
library(caret)
library(modelsummary)
library(grid)
library(kableExtra)
# Getting the data
data <- read.csv( 'https://osf.io/4ay9x/download', stringsAsFactors = TRUE)
```

## Introduction

In this report I showcase 4 models, that predict the earnings per hour of Educational Administrators (occupational code 0230). The models have different levels of complexity, model 1 being the most simple, while model 4 contains the most predictor variables. The aim of this report is to find the model with the best performance based on RMSE in full sample, cross-validated RMSE and BIC in full sample. 


## Data

The used for the report is the [cps-earnings dataset](https://osf.io/4ay9x/).
As mentioned above, I chose the Educational Administrators (occupational code 0230) [occupation](https://osf.io/57n9q/) for the prediction. 


```{r, echo=FALSE}
# selecting occupation 230 - Education administrators
data <- data %>% filter(occ2012 == 230)

##### adding variables ####
data <- data %>% mutate(female=as.numeric(sex==2)) %>%
  mutate(w=earnwke/uhours) %>%
  mutate(agesq=age^2) %>% 
  mutate(unionmember=ifelse(unionmme=='Yes',1,0))
```

The number of observations is `r sum(!is.na(data$X))` for all of our key variables.

```{r, echo=FALSE, warning=FALSE, fig.width=8, fig.height = 3, fig.align="center" }
# Linear regressions
model1 <- as.formula(w ~ age + agesq)
model2 <- as.formula(w ~ age + agesq + female)
model3 <- as.formula(w ~ age + agesq + female + grade92)
model4 <- as.formula(w ~ age + agesq + female + grade92 + unionmember + marital + ownchild)

# Running simple OLS
reg1 <- feols(model1, data=data, vcov = 'hetero')
reg2 <- feols(model2, data=data, vcov = 'hetero')
reg3 <- feols(model3, data=data, vcov = 'hetero')
reg4 <- feols(model4, data=data, vcov = 'hetero')

# evaluation of the models
fitstat_register("k", function(x){length( x$coefficients ) - 1}, "No. Variables")
kable( etable( reg1 , reg2 , reg3 , reg4 , fitstat = c('bic','rmse','n','k') ),
        col.names = c('(1)','(2)','(3)','(4)'),
       "latex", booktabs = TRUE,  position = "H",
       caption = 'Evaluation of the models') %>% kable_styling(latex_options = c("hold_position","scale_down"))

```

DESCRIPTION OF THE FIGURE. WHAT DOES IT TELS US?

(May change the order of descriptive stats and graph.)

The key pattern of association is:

```{r, echo=FALSE}
####### Cross-validation ########

# Setting number of fold
k <- 5

set.seed(43502)
cv1 <- train(model1, data, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(43502)
cv2 <- train(model2, data, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(43502)
cv3 <- train(model3, data, method = "lm", trControl = trainControl(method = "cv", number = k))
set.seed(43502)
cv4 <- train(model4, data, method = "lm", trControl = trainControl(method = "cv", number = k))

# Calculating RMSE for each fold and the average RMSE as well
cv <- c("cv1", "cv2", "cv3", "cv4")
rmse_cv <- c()

for(i in 1:length(cv)){
  rmse_cv[i] <- sqrt((get(cv[i])$resample[[1]][1]^2 +
                        get(cv[i])$resample[[1]][2]^2 +
                        get(cv[i])$resample[[1]][3]^2 +
                        get(cv[i])$resample[[1]][4]^2)/4)
}

# summarize results
cv_mat <- data.frame(rbind(cv1$resample[4], "Average"),
                     rbind(cv1$resample[1], rmse_cv[1]),
                     rbind(cv2$resample[1], rmse_cv[2]),
                     rbind(cv3$resample[1], rmse_cv[3]),
                     rbind(cv4$resample[1], rmse_cv[4])
)

colnames(cv_mat)<-c("Resample","Model1", "Model2", "Model3", "Model4")
cv_mat 

```

How will you include this in your model?

Short description on the other variables: 2-10 sentence depends on the amount of variables you have. You should reference your decisions on the graphs/analysis which are located in the appendix.

## Model


```{r, echo = FALSE, warning=FALSE, fig.width=4, fig.height = 3, fig.align="center" }
# Show model complexity and out-of-sample RMSE performance
m_comp <- c()
models <- c("reg1", "reg2", "reg3", "reg4")
for( i in 1 : length(cv) ){
  m_comp[ i ] <- length( get( models[i] )$coefficient  - 1 ) 
}

m_comp <- tibble( model = models , 
                  complexity = m_comp,
                  RMSE = rmse_cv )

ggplot( m_comp , aes( x = complexity , y = RMSE ) ) +
  geom_point(color='red',size=2) +
  geom_line(color='blue',size=0.5)+
  labs(x='Number of explanatory variables',y='Averaged RMSE on test samples',
       title='Prediction performance and model compexity') +
  theme_bw()
```






## Robustness check / 'Heterogeneity analysis'

Task: calculate and report t-tests for each countries. 



## Conclusion

HERE COMES WHAT WE HAVE LEARNED AND WHAT WOULD STRENGHTEN AND WEAKEN OUR ANALYSIS.

## Appendix

Here comes all the results which are referenced and not essential for understanding the MAIN results.