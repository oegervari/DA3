---
title: "DA3 - Assignment 3"
subtitle: "Technical Report"
author: "Oszkar Egervari"
date: '2022-02-18'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, include=F}
#### SET UP
rm(list=ls())

# Import libraries
library(haven)
library(glmnet)
library(purrr)
library(margins)
library(skimr)
library(kableExtra)
library(Hmisc)
library(cowplot)
library(gmodels) 
library(lspline)
library(sandwich)
library(modelsummary)

library(rattle)
library(caret)
library(pROC)
library(ranger)
library(rpart)
library(partykit)
library(rpart.plot)
library(viridis)


path <- "~/Documents/CEU/Winter_term/DA_3/DA3/assignment_3/"


# set data dir, data used
source(paste0(path, "theme_bg.R"))             
source(paste0(path, "da_helper_functions.R"))


# Load the data
data <- readRDS(paste0(path,"bisnode_firms_clean.rds"))

options(digits = 4)

```


```{r, include=F}
# Define variable sets -----------------------------------------------------------------------

rawvars <-  c("curr_assets", "curr_liab", "extra_exp", "extra_inc", "extra_profit_loss", "fixed_assets",
              "inc_bef_tax", "intang_assets", "inventories", "liq_assets", "material_exp", "personnel_exp",
              "profit_loss_year", "sales", "share_eq", "subscribed_cap")
engvar <- c("total_assets_bs", "fixed_assets_bs", "liq_assets_bs", "curr_assets_bs",
            "share_eq_bs", "subscribed_cap_bs", "intang_assets_bs", "extra_exp_pl",
            "extra_inc_pl", "extra_profit_loss_pl", "inc_bef_tax_pl", "inventories_pl",
            "material_exp_pl", "profit_loss_year_pl", "personnel_exp_pl")
engvar2 <- c("extra_profit_loss_pl_quad", "inc_bef_tax_pl_quad",
             "profit_loss_year_pl_quad", "share_eq_bs_quad")
engvar3 <- c(grep("*flag_low$", names(data), value = TRUE),
             grep("*flag_high$", names(data), value = TRUE),
             grep("*flag_error$", names(data), value = TRUE),
             grep("*flag_zero$", names(data), value = TRUE))
d1 <-  c("d1_sales_mil_log_mod", "d1_sales_mil_log_mod_sq",
         "flag_low_d1_sales_mil_log", "flag_high_d1_sales_mil_log")
hr <- c("female", "ceo_age", "flag_high_ceo_age", "flag_low_ceo_age",
        "flag_miss_ceo_age", "ceo_count", "labor_avg_mod",
        "flag_miss_labor_avg", "foreign_management")
firm <- c("age", "age2", "new", "ind2_cat", "m_region_loc", "urban_m")

# interactions for logit, LASSO
interactions1 <- c("ind2_cat*age", "ind2_cat*age2",
                   "ind2_cat*d1_sales_mil_log_mod", "ind2_cat*sales_mil_log",
                   "ind2_cat*ceo_age", "ind2_cat*foreign_management",
                   "ind2_cat*female",   "ind2_cat*urban_m", "ind2_cat*labor_avg_mod")
interactions2 <- c("sales_mil_log*age", "sales_mil_log*female",
                   "sales_mil_log*profit_loss_year_pl", "sales_mil_log*foreign_management")


X1 <- c("sales_mil_log", "sales_mil_log_sq", "d1_sales_mil_log_mod", "profit_loss_year_pl", "ind2_cat")
X2 <- c("sales_mil_log", "sales_mil_log_sq", "d1_sales_mil_log_mod", "profit_loss_year_pl", "ind2_cat", "fixed_assets_bs","share_eq_bs","curr_liab_bs ",   "curr_liab_bs_flag_high ", "curr_liab_bs_flag_error",  "age","foreign_management")
X3 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, d1)
X4 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, engvar2, engvar3, d1, hr)
X5 <- c("sales_mil_log", "sales_mil_log_sq", firm, engvar, engvar2, engvar3, d1, hr, interactions1, interactions2)

# for LASSO
logitvars <- c("sales_mil_log", "sales_mil_log_sq", engvar, engvar2, engvar3, d1, hr, firm, interactions1, interactions2)

# for RF (no interactions, no modified features)
rfvars  <-  c("sales_mil", "d1_sales_mil_log", rawvars, hr, firm)
```


## Introduction

The goal of this project is to help investors finding companies with a potential for fast growth. To achieve this I built various prediction models on a dataset consisting of companies in the European Union. The data includes all registered companies from 2005-2016 in three selected industries (auto manufacturing, equipment manufacturing, hotels and restaurants). 
Several features were used during the prediction, such as P/L statements, balance sheets, management information etc. 
I considered a company fast growing when its compound annual growth rate (CAGR) exceeded 35%. This is an arbitrary number, however after some research I found out that between 15% and 25% CAGR, investors consider a company to be a 'good investment' in general, so I wanted to overshoot that value by a bit. 

Link to project [Github](https://github.com/oegervari/da3_a3) repo.


## The Data 

The data set was constructed by  Bisnode, a business data and analytics company (www.bisnode.com) and includes the detailed company data from a middle-sized country in the European Union. All registered companies are from 2005-2016 in three selected industries of auto manufacturing, equipment manufacturing, hotels and restaurants.

The data set has 287 829 observations (46 412 firms) and 48 variables describing all available information like properties of company, balance sheet, profit and loss elements and management information. As mentioned above, the CAGR was calculated to help us decide whether a company qualifies as fast growing. 

The analysis is conducted for a period between 2011 and 2013. The CAGR of sales is calculated from 2012 to 2013 and a change of sales variable between 2011 to 2012 was created. In addition log of sales variable was created via applying logarithmic transformation on sales. 

```{r  message=FALSE, warning=FALSE, echo=FALSE, out.width = '50%', fig.height=4}

ggplot(data=data, aes(x=sales_mil)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.1,
                 color = "white", fill = "green") +
  coord_cartesian(xlim = c(0, 5)) +
  labs(title="Distribution of sales (2012)", x = "sales in million",y = "Percent")+
  theme_minimal() 

ggplot(data=data, aes(x=sales_mil_log)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 0.25,
                 color = "white", fill = "green") +
  labs(title="Distribution of log sales (2012)", x = "log sales in million",y = "Percent")+
  theme_minimal()

```

To create a proper analysis we filtered the data for companies with full balance sheet for these 3 years and with a sales number exceeding 0. On top of that, I excluded companies with a sales value (annual) below 1000 and above 10 million Euros. 
At this point we are left with 59 538 observations. As a last step, the year is set to 2012 and - after creating the cagr_sales variable - companies with cagr_sales values exceeding 5000 or NAs are filtered out, which leaves us with 14 628 observations.
The distribution of CAGR of sales can be seen below.

```{r  message=FALSE, warning=FALSE, echo=FALSE, fig.align="center",out.width = '50%', fig.height=4}

# Distribution of CAGR sales
ggplot(data=data, aes(x=cagr_sales)) +
  geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = 10, boundary=0,
                 color = "white", fill = "green") +
  coord_cartesian(xlim = c(-100, 200)) +
  labs(title="Distribution of CAGR sales (2012 to 2013)", x = "CAGR",y = "Percent")+
  theme_minimal() 

```

### Feature engineering
During this task I take a look at financial variables and their validity. Variables were flagged with mistakes in their balance sheet (e.g. negative values in assets), also category variables and  factors were created. Finally I dropped observations with too many missing values in key variables and we finished with 116 variables and 13 099 observations.

9 groups were created for modelling purposes:

- **Raw variables:** our key variables for the prediction, such as main profit and loss and balance sheet elements. 
- **Engine variables 1:** other profit and loss and balance sheet elements.
- **Engine variables 2:** quadratic transformation of some key variables, such as profit and loss, income before tax, extra profit and loss, share of equity. These variables are mostly between -1 and 1.
- **Engine variables 3:** flags of engine 2 variables.
- **D1:** variables measuring change of sales.
- **HR:** data about management such as age and gender of CEO or average labor number. 
- **Firm:** properties of the firms, like age of the company, region, etc.
- **Interactions 1 and 2:** interactions of variables.


# Modelling

```{r, echo=F, warning=F, message=F}

set.seed(2022)

train_indices <- as.integer(createDataPartition(data$fast_growth, p = 0.8, list = FALSE))
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]

data %>% 
  group_by(fast_growth_f) %>% 
  summarise("Number of companies" = n(), "Percentage" = paste0(round(n()/13099*100),'%')) %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
  
# 5 fold cross-validation ----------------------------------------------------------------------
train_control <- trainControl(
  method = "cv",
  number = 5,
  classProbs = TRUE,
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)

```

### Splitting the data

The first step before building the models is to split to data to train and holdout sets. The holdout set consists of 20% of the whole set. The selection is random of course. The remaining data is the training set. The holdout set is used for the final evaluation of the models. 
The training set is again divided into smaller parts with 5-fold cross validation for each model.

### I. Probability logit models
First I build 5 different logit models. Each model contains a different set of variables increasing in complexity, meaning that model 1 is the least complex, while model 5 is the most.


```{r, echo=F, message=F, warning=F}
models <- data.frame(row.names = c("X1 model", "X2 model", "X3 model", "X4 model", "X5 model"))
models$Variables[1] <- "Log sales + Log sales^2 + Change in Sales + Profit and loss + Industry"
models$Variables[2] <- "X1 + Fixed assets + Equity + Current liabilities (and flags) + Age + Foreign management"
models$Variables[3] <- "Log sales + Log sales^2 + Firm + Engine variables 1 + D1"
models$Variables[4] <- "X3 + Engine variables 2 + Engine variables 3 + HR"
models$Variables[5] <- "X4 + Interactions 1 and 2"

models %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```


```{r, include=F}
# Logit Models ----------------------------------------------
logit_model_vars <- list("X1" = X1, "X2" = X2, "X3" = X3, "X4" = X4, "X5" = X5)

CV_RMSE_folds <- list()
logit_models <- list()

for (model_name in names(logit_model_vars)) {

  features <- logit_model_vars[[model_name]]

  set.seed(2022)
  glm_model <- train(
    formula(paste0("fast_growth_f ~", paste0(features, collapse = " + "))),
    method = "glm",
    data = data_train,
    family = binomial,
    trControl = train_control
  )

save( glm_model , file = 'glm_model.RData' )
# load(url('https://github.com/regulyagoston/BA21_Coding/blob/main/Class_3/data/rf_model_1.RData?raw=true'))

  
  logit_models[[model_name]] <- glm_model
  # Calculate RMSE on test for each fold
  CV_RMSE_folds[[model_name]] <- glm_model$resample[,c("Resample", "RMSE")]}
  
# LASSO ---------------------------------------------------------
lambda <- 10^seq(-1, -4, length = 10)
grid <- expand.grid("alpha" = 1, lambda = lambda)

set.seed(2022)
system.time({
  logit_lasso_model <- train(
    formula(paste0("fast_growth_f ~", paste0(logitvars, collapse = " + "))),
    data = data_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = grid,
    na.action=na.exclude
  )
})

tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
logit_models[["LASSO"]] <- logit_lasso_model
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))

CV_RMSE_folds[["LASSO"]] <- logit_lasso_model$resample[,c("Resample", "RMSE")]


```

Comparing the models, the two most important measures were the Root Mean Squared Error (RMSE) and the Area Under Curve (AUC), both averaged on the 5 different folds used during cross validation. We can see from the table below that the RMSE and AUC values are really similar for all of the models. The best model that had the lowest RMSE was the third one (X3), but the best model that has the highest AUC was the fourth one (X4). These two models outperformed all the others based on these criterion. I will consider the X3 model as a benchmark as it is simpler with less than half of the predictors the 4th model possesses.  

```{r, echo=F, message=F, warning=F}
# Draw ROC Curve and calculate AUC for each folds --------------------------------
CV_AUC_folds <- list()

for (model_name in names(logit_models)) {

  auc <- list()
  model <- logit_models[[model_name]]
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)

    roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
    auc[[fold]] <- as.numeric(roc_obj$auc)
  }

  CV_AUC_folds[[model_name]] <- data.frame("Resample" = names(auc),
                                              "AUC" = unlist(auc))
}

# For each model: average RMSE and average AUC for models ----------------------------------

CV_RMSE <- list()
CV_AUC <- list()

for (model_name in names(logit_models)) {
  CV_RMSE[[model_name]] <- mean(CV_RMSE_folds[[model_name]]$RMSE)
  CV_AUC[[model_name]] <- mean(CV_AUC_folds[[model_name]]$AUC)
}

# We have 6 models, (5 logit and the logit lasso). For each we have a 5-CV RMSE and AUC.
# We pick our preferred model based on that. -----------------------------------------------

nvars <- lapply(logit_models, FUN = function(x) length(x$coefnames))
nvars[["LASSO"]] <- sum(lasso_coeffs != 0)

logit_summary1 <- data.frame("Number of predictors" = unlist(nvars),
                             "CV RMSE" = unlist(CV_RMSE),
                             "CV AUC" = unlist(CV_AUC))


logit_summary1 %>% 
  slice(1:5) %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```

### II. LASSO model
After the basic logit models, by applying LASSO the variables are selected based on calculations, rather than being handpicked. The LASSO model contains all the variables of the 5th logit model. Out of those, only 24 were selected opposed to the 149 predictors of model 5, meanwhile the RMSE slightly improved. However the AUC declined compared to model 3, 4 and 5.

```{r, echo=F, message=F, warning=F}
logit_summary1 %>% 
  slice(c(3,6)) %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```

### III. Random forest
After the logit and LASSO models, I built a random forest model, since it's good at finding non-linear patterns and interactions. The variables of the fourth model were used without adding polynomials, flag variables etc. In the tuning grid I used the default of growing 500 trees with 10 and 15 as the minimum number of observations at each terminal node, and 5, 6 or 7 variables to use for each split.
The random forest outperformed our best model, the X3 logit model, both in terms of lower RMSE with 0.372 and higher AUC with 0.691 values.

```{r, include=F}
# 5 fold cross-validation

train_control <- trainControl(
  method = "cv",
  n = 5,
  classProbs = TRUE, # same as probability = TRUE in ranger
  summaryFunction = twoClassSummaryExtended,
  savePredictions = TRUE
)
train_control$verboseIter <- TRUE

tune_grid <- expand.grid(
  .mtry = c(5, 6, 7),
  .splitrule = "gini",
  .min.node.size = c(10, 15)
)

# build rf model
# set.seed(2022)
# rf_model_p <- train(
#   formula(paste0("fast_growth_f ~ ", paste0(rfvars , collapse = " + "))),
#   method = "ranger",
#   data = data_train,
#   tuneGrid = tune_grid,
#   trControl = train_control,
# )

# ( rf_model_p , file = 'rf_model_p.RData' )
load(url('https://github.com/oegervari/da3_a3/blob/main/code/rf_model_p.RData?raw=true'))

best_mtry <- rf_model_p$bestTune$mtry
best_min_node_size <- rf_model_p$bestTune$min.node.size
```


```{r, echo=F, message=F, warning=F}

# Get average (ie over the folds) RMSE and AUC ------------------------------------
CV_RMSE_folds[["rf_p"]] <- rf_model_p$resample[,c("Resample", "RMSE")]

auc <- list()
for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p$pred %>%
    filter(Resample == fold)
  
  roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
  auc[[fold]] <- as.numeric(roc_obj$auc)
}
CV_AUC_folds[["rf_p"]] <- data.frame("Resample" = names(auc),
                                     "AUC" = unlist(auc))

CV_RMSE[["Random_forest"]] <- mean(CV_RMSE_folds[["rf_p"]]$RMSE)
CV_AUC[["Random_forest"]] <- mean(CV_AUC_folds[["rf_p"]]$AUC)


rf_summary <- data.frame("CV RMSE" = unlist(CV_RMSE),
                         "CV AUC" = unlist(CV_AUC))

rf_summary %>% 
  slice(c(3,7)) %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

```

### ROC curve


Before finding the best thresholds for our prediction, I drew up the ROC plots for the best models so far, the random forest and the X3 logit model. The threshold values range between 0.05 and 0.75 with an increment of 0.025. This shows us how the increasing threshold leads to lower True positive and False positive rates. The second plot shows the Area Under the Curve. 
There is a trade off between accurate true positive and true negative predictions. Setting up the loss function helps us find the optimal threshold.

```{r, echo=F, message=F, warning=F, out.width="50%"}

best_no_loss <- rf_model_p

predicted_probabilities_holdout <- predict(best_no_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_no_loss_pred"] <- predicted_probabilities_holdout[,"fast_growth"]

# discrete ROC (with thresholds in steps) on holdout -------------------------------------------------
thresholds <- seq(0.05, 0.75, by = 0.025)

cm <- list()
true_positive_rates <- c()
false_positive_rates <- c()
for (thr in thresholds) {
  holdout_prediction <- ifelse(data_holdout[,"best_no_loss_pred"] < thr, "no_fast_growth", "fast_growth") %>%
    factor(levels = c("no_fast_growth", "fast_growth"))
  cm_thr <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_f)$table
  cm[[as.character(thr)]] <- cm_thr
  true_positive_rates <- c(true_positive_rates, cm_thr["fast_growth", "fast_growth"] /
                             (cm_thr["fast_growth", "fast_growth"] + cm_thr["no_fast_growth", "fast_growth"]))
  false_positive_rates <- c(false_positive_rates, cm_thr["fast_growth", "no_fast_growth"] /
                              (cm_thr["fast_growth", "no_fast_growth"] + cm_thr["no_fast_growth", "no_fast_growth"]))
}

tpr_fpr_for_thresholds <- tibble(
  "threshold" = thresholds,
  "true_positive_rate" = true_positive_rates,
  "false_positive_rate" = false_positive_rates
)

ggplot(
  data = tpr_fpr_for_thresholds,
  aes(x = false_positive_rate, y = true_positive_rate, color = threshold)) +
  labs(x = "False positive rate (1 - Specificity)", y = "True positive rate (Sensitivity)") +
  geom_point(size=2, alpha=0.8) +
  scale_color_viridis(option = "D", direction = -1) +
  scale_x_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  theme_bw() +
  theme(legend.position ="right") +
  theme(legend.title = element_text(size = 4), 
        legend.text = element_text(size = 4),
        legend.key.size = unit(.4, "cm")) 



# continuous ROC on holdout with best model (Logit 4) -------------------------------------------
roc_obj_holdout <- roc(data_holdout$fast_growth, data_holdout$best_no_loss_pred)

createRocPlot(roc_obj_holdout, "best_no_loss_roc_plot_holdout")


```

```{r, echo=F, message=F, warning=F, out.width="50%"}

best_no_loss <- logit_models$X3

predicted_probabilities_holdout <- predict(best_no_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_no_loss_pred"] <- predicted_probabilities_holdout[,"fast_growth"]

# discrete ROC (with thresholds in steps) on holdout -------------------------------------------------
thresholds <- seq(0.05, 0.75, by = 0.025)

cm <- list()
true_positive_rates <- c()
false_positive_rates <- c()
for (thr in thresholds) {
  holdout_prediction <- ifelse(data_holdout[,"best_no_loss_pred"] < thr, "no_fast_growth", "fast_growth") %>%
    factor(levels = c("no_fast_growth", "fast_growth"))
  cm_thr <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_f)$table
  cm[[as.character(thr)]] <- cm_thr
  true_positive_rates <- c(true_positive_rates, cm_thr["fast_growth", "fast_growth"] /
                             (cm_thr["fast_growth", "fast_growth"] + cm_thr["no_fast_growth", "fast_growth"]))
  false_positive_rates <- c(false_positive_rates, cm_thr["fast_growth", "no_fast_growth"] /
                              (cm_thr["fast_growth", "no_fast_growth"] + cm_thr["no_fast_growth", "no_fast_growth"]))
}

tpr_fpr_for_thresholds <- tibble(
  "threshold" = thresholds,
  "true_positive_rate" = true_positive_rates,
  "false_positive_rate" = false_positive_rates
)

ggplot(
  data = tpr_fpr_for_thresholds,
  aes(x = false_positive_rate, y = true_positive_rate, color = threshold)) +
  labs(x = "False positive rate (1 - Specificity)", y = "True positive rate (Sensitivity)") +
  geom_point(size=2, alpha=0.8) +
  scale_color_viridis(option = "D", direction = -1) +
  scale_x_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  scale_y_continuous(expand = c(0.01,0.01), limit=c(0,1), breaks = seq(0,1,0.1)) +
  theme_bw() +
  theme(legend.position ="right") +
  theme(legend.title = element_text(size = 4), 
        legend.text = element_text(size = 4),
        legend.key.size = unit(.4, "cm")) 



# continuous ROC on holdout with best model (Logit 4) -------------------------------------------
roc_obj_holdout <- roc(data_holdout$fast_growth, data_holdout$best_no_loss_pred)

createRocPlot(roc_obj_holdout, "best_no_loss_roc_plot_holdout")


```


### Finding the optimal threshold

To find the optimal threshold, we have to weigh the consequences of a false negative and a false positive prediction. In case of a false positive prediction, one would make a bad investment. It can mean acquiring less return compared to an alternative investment, but also potentially losing money. On the other hand, a false negative prediction would would result in loosing out on a good investment. Considering all this, my personal opinion is that a false positive prediction has less downside, than a false negative, so I set the loss function to a 1/2 FP/FN ratio, meaning that a false negative prediction costs twice as much as a false positive.


```{r, echo=F, message=F, warning=F}
FP=1
FN=2
cost = FN/FP
# the prevalence, or the proportion of cases in the population (n.cases/(n.controls+n.cases))
prevelance = sum(data_train$fast_growth)/length(data_train$fast_growth)

# LOGIT AND LASSO ------------------------------------------------------------------------------
# Draw ROC Curve and find optimal threshold with loss function --------------------------

best_tresholds <- list()
expected_loss <- list()
logit_cv_rocs <- list()
logit_cv_threshold <- list()
logit_cv_expected_loss <- list()

for (model_name in names(logit_models)) {
  
  model <- logit_models[[model_name]]
  colname <- paste0(model_name,"_prediction")
  
  best_tresholds_cv <- list()
  expected_loss_cv <- list()
  
  for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
    cv_fold <-
      model$pred %>%
      filter(Resample == fold)
    
    roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
    best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                            best.method="youden", best.weights=c(cost, prevelance))
    best_tresholds_cv[[fold]] <- best_treshold$threshold
    expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$fast_growth)
  }
  
  # average
  best_tresholds[[model_name]] <- mean(unlist(best_tresholds_cv))
  expected_loss[[model_name]] <- mean(unlist(expected_loss_cv))
  
  # for fold #5
  logit_cv_rocs[[model_name]] <- roc_obj
  logit_cv_threshold[[model_name]] <- best_treshold
  logit_cv_expected_loss[[model_name]] <- expected_loss_cv[[fold]]
  
}

# RANDOM FOREST --------------------------------------------------------------------------
# Now use loss function and search for best thresholds and expected loss over folds -----
best_tresholds_cv <- list()
expected_loss_cv <- list()

for (fold in c("Fold1", "Fold2", "Fold3", "Fold4", "Fold5")) {
  cv_fold <-
    rf_model_p$pred %>%
    filter(mtry == best_mtry,
           min.node.size == best_min_node_size,
           Resample == fold)
  
  roc_obj <- roc(cv_fold$obs, cv_fold$fast_growth)
  best_treshold <- coords(roc_obj, "best", ret="all", transpose = FALSE,
                          best.method="youden", best.weights=c(cost, prevelance))
  best_tresholds_cv[[fold]] <- best_treshold$threshold
  expected_loss_cv[[fold]] <- (best_treshold$fp*FP + best_treshold$fn*FN)/length(cv_fold$fast_growth)
}

# average
best_tresholds[["rf_p"]] <- mean(unlist(best_tresholds_cv))
expected_loss[["rf_p"]] <- mean(unlist(expected_loss_cv))


# Save output --------------------------------------------------------
# Model selection is carried out on this CV RMSE

nvars[["rf_p"]] <- length(rfvars)

summary_results <- data.frame("Number of predictors" = unlist(nvars),
                              "CV RMSE" = unlist(CV_RMSE),
                              "CV AUC" = unlist(CV_AUC),
                              "CV threshold" = unlist(best_tresholds),
                              "CV expected Loss" = unlist(expected_loss))

model_names <- c("Logit X1", "Logit X3", "Logit X4",
                 "Logit LASSO","RF probability")
summary_results <- summary_results %>%
  filter(rownames(.) %in% c("X1", "X3", "X4", "LASSO", "rf_p"))
rownames(summary_results) <- model_names

summary_results %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```

### Model choice
The choice for the final model was mainly based on the expected losses. Here the lowest expected loss is for the random forest, then the X3 and X4 logit models, finally the LASSO and least complex first model. 
Ordering the models based on AUC results in the same list.
RMSE results in a slightly different order with LASSO having a better score than model X4.

Even though the random forest was the best for all the comparison measures, I ended up going with the less complex X3 logit model. The numbers are close, but X3 is a much simpler model, having less variables than the LASSO model and it is easily to interpret compared to the black box random forest model. On top of that, the random forest runs significantly longer than the other models.

### Model evaluation
Now that we have the optimal threshold vales we can turn to the holdout set and evaluate our chosen best model the logit X3. The expected loss we get when calculating it on the holdout set is **0.325**. This is even smaller than what we got for the train data. 
Another way of checking the performance of our model is taking a look at the confusion matrix. Based on that, the model accuracy is 81.3% and its sensitivity is 93.7%.



```{r, echo=F, message=F, warning=F}

best_logit_with_loss <- logit_models[["X3"]]
best_logit_optimal_treshold <- best_tresholds[["X3"]]

logit_predicted_probabilities_holdout <- predict(best_logit_with_loss, newdata = data_holdout, type = "prob")
data_holdout[,"best_logit_with_loss_pred"] <- logit_predicted_probabilities_holdout[,"fast_growth"]

# ROC curve on holdout
roc_obj_holdout <- roc(data_holdout$fast_growth, data_holdout[, "best_logit_with_loss_pred", drop=TRUE])

# Get expected loss on holdout
holdout_treshold <- coords(roc_obj_holdout, x = best_logit_optimal_treshold, input= "threshold",
                           ret="all", transpose = FALSE)
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(data_holdout$fast_growth)


# Confusion table on holdout with optimal threshold
holdout_prediction <-
  ifelse(data_holdout$best_logit_with_loss_pred < best_logit_optimal_treshold, "no_fast_growth", "fast_growth") %>%
  factor(levels = c("no_fast_growth", "fast_growth"))
cm_object3 <- confusionMatrix(holdout_prediction,data_holdout$fast_growth_f)
cm3 <- cm_object3$table

cm3 %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")

# Take model to holdout and estimate RMSE, AUC and expected loss ---------

rf_predicted_probabilities_holdout <- predict(logit_models$X3, newdata = data_holdout, type = "prob")
data_holdout$rf_p_prediction <- rf_predicted_probabilities_holdout[,"fast_growth"]


# ROC curve on holdout
roc_obj_holdout <- roc(data_holdout$fast_growth, data_holdout[, "rf_p_prediction", drop=TRUE])


# Get expected loss on holdout with optimal threshold
holdout_treshold <- coords(roc_obj_holdout, x = best_tresholds[["rf_p"]] , input= "threshold",
                           ret="all", transpose = FALSE)
expected_loss_holdout <- (holdout_treshold$fp*FP + holdout_treshold$fn*FN)/length(data_holdout$fast_growth)


test_results<-data.frame(
  model="X3 Logit Model", 
  `Test RMSE`= RMSE(data_holdout$rf_p_prediction, data_holdout$fast_growth),
  `Test AUC`= as.numeric(roc_obj_holdout$auc), 
  `Avg. Exp. Loss`=expected_loss_holdout)

test_results %>% 
  kbl() %>% 
  kable_classic(full_width = F, html_font = "Cambria")
```
